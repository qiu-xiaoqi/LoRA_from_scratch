{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: einops in d:\\miniconda3\\envs\\llm\\lib\\site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda\n",
      "dtype:torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from einops import rearrange\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "print(f'device:{device}\\ndtype:{dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建一个llama小型模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.for_model('llama') # 核心配置\n",
    "config.hidden_size = 24\n",
    "config.intermediate_size = config.hidden_size * 4\n",
    "config.num_attention_heads = 4\n",
    "config.num_hidden_layers = 4\n",
    "config.num_key_value_heads = 2\n",
    "config.vocab_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128, 24)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=24, out_features=24, bias=False)\n",
       "        (k_proj): Linear(in_features=24, out_features=12, bias=False)\n",
       "        (v_proj): Linear(in_features=24, out_features=12, bias=False)\n",
       "        (o_proj): Linear(in_features=24, out_features=24, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (up_proj): Linear(in_features=24, out_features=96, bias=False)\n",
       "        (down_proj): Linear(in_features=96, out_features=24, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_model = AutoModel.from_config(config)\n",
    "raw_model # 可以将输出结果与模型图对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLinear(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_layer: nn.Linear, # 原来的线性层\n",
    "            r: int = 8, # lora Rank\n",
    "            alpha: int = 16, # lora alpha\n",
    "            dropout_p: float = 0.0, # lora dropout\n",
    "            test_mode: bool = False, # 测试模式，用于控制lora_B 是否为全零\n",
    "    ):\n",
    "        super(LoraLinear, self).__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # 定义lora_A 和 lora_B 为 parameter\n",
    "        self.lora_A = nn.Parameter(torch.empty((r, base_layer.in_features), dtype=base_layer.weight.dtype))\n",
    "        self.lora_B = nn.Parameter(torch.empty((base_layer.out_features, r), dtype=base_layer.weight.dtype))\n",
    "\n",
    "        # 初始化lora矩阵\n",
    "        nn.init.normal_(self.lora_A, mean=0.0, std=0.02)\n",
    "        if test_mode:\n",
    "            nn.init.normal_(self.lora_B, mean=0.0, std=0.02)\n",
    "        else:\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "        for parma in self.base_layer.parameters():\n",
    "            parma.requires_grad = False\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        scaling = float(self.alpha) / float(self.r) # lora缩放系数\n",
    "        lora_adjustment = F.linear(self.dropout(x), self.lora_A)\n",
    "        lora_adjustment = F.linear(lora_adjustment, self.lora_B)\n",
    "        # 相当于 base_layer(x) + lora_A @ lora_B\n",
    "        return self.base_layer(x) + lora_adjustment * scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(\n",
    "        module: nn.Module,\n",
    "        r: int = 8,\n",
    "        alpha: int = 16,\n",
    "        dropout_p: float = 0.0,\n",
    "        embed_requires_grad: bool = False, # embedding 层是否训练\n",
    "        norm_requires_grad: bool = False, # norm 层是否训练\n",
    "        head_requires_grad: bool = False, # head 层是否训练(Causal LM 才有)\n",
    "        test_mode: bool = False, # 是否测试模式,用于控制lora_B 是否全为0\n",
    "):\n",
    "    \"\"\"\n",
    "    找到module中所有线性层并递归替换\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        # 先处理额外的层, lm_head也是linear, 所以先处理\n",
    "        if any(s in name for s in ['embed', 'norm', 'lm_head']):\n",
    "            requires_grad = embed_requires_grad if 'embed' in name \\\n",
    "                            else norm_requires_grad if 'norm' in name \\\n",
    "                            else head_requires_grad\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "        # 替换所有线性层, QLoRA做法\n",
    "        elif isinstance(child, nn.Linear):\n",
    "            lora_linear = LoraLinear(child, r=r, alpha=alpha, dropout_p=dropout_p, test_mode=test_mode)\n",
    "            setattr(module, name, lora_linear)\n",
    "        # 递归向下替换\n",
    "        else:\n",
    "            replace_linear_with_lora(\n",
    "                child,r,alpha,dropout_p,\n",
    "                embed_requires_grad,norm_requires_grad,head_requires_grad,test_mode=test_mode\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_lora(module: nn.Module, adapter_name: str='adapter'):\n",
    "    \"\"\"\n",
    "    卸载lora参数, 并将原模型恢复至加载lora前的样子\n",
    "    \"\"\"\n",
    "    lora_parameters = {}\n",
    "    def search_lora_linear(module: nn.Module, pefix:List[str]):\n",
    "        for name, child in module.named_children():\n",
    "            new_prefix = pefix + [name]\n",
    "            if isinstance(child, LoraLinear):\n",
    "                # 保存lora参数\n",
    "                lora_parameters['.'.join(new_prefix)] = {\n",
    "                    \"lora_A_weight\": child.lora_A.data.cpu(),\n",
    "                    \"lora_B_weight\": child.lora_B.data.cpu(),\n",
    "                    \"r\": child.r,\n",
    "                    \"alpha\": child.alpha,\n",
    "                    \"dropout_p\": child.dropout.p\n",
    "                }\n",
    "                setattr(module, name, child.base_layer)\n",
    "            else:\n",
    "                search_lora_linear(child, new_prefix)\n",
    "        \n",
    "    search_lora_linear(module, [])\n",
    "    # 解冻原模型\n",
    "    for name, param in module.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    torch.save(lora_parameters, f\"{adapter_name}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora(module: nn.Module, adapter_name: str = 'adapter'):\n",
    "    \"\"\"\n",
    "    加载lora参数\n",
    "    \"\"\"\n",
    "    lora_parameters = torch.load(f\"{adapter_name}.pt\")\n",
    "\n",
    "    for name, lora_params in lora_parameters.items():\n",
    "        child = dict(module.named_modules())[name]\n",
    "        if isinstance(child, nn.Linear):\n",
    "            lora_linear = LoraLinear(child, lora_params['r'], lora_params['alpha'], lora_params['dropout_p'])\n",
    "            lora_linear.lora_A.data = lora_params['lora_A_weight'].to(lora_linear.lora_A.device)\n",
    "            lora_linear.lora_B.data = lora_params['lora_B_weight'].to(lora_linear.lora_B.device)\n",
    "\n",
    "            # 名称示例： layers().self_attn.q_proj\n",
    "            parts = name.split('.')\n",
    "            obj = module\n",
    "            for part in parts[:-1]: # 不包括最后一级\n",
    "                obj = getattr(obj, part)\n",
    "            setattr(obj, parts[-1], lora_linear)\n",
    "    \n",
    "    # 恢复原来的冻结方式，这里简单地除了lora全冻结\n",
    "    for name, param in module.named_parameters():\n",
    "        if any(s in name for s in ['embed', 'norm', 'lm_head']):\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model: nn.Module):\n",
    "    \"\"\"\n",
    "    打印可训练参数，和PeftModel的方法类似\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    trainable_percentage = 100 * trainable_params / total_params\n",
    "\n",
    "    print(f\"trainable params: {trainable_params:,} || all params: {total_params:,} || trainable%: {trainable_percentage:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 37,848 || all params: 37,848 || trainable%: 100.0000\n",
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128, 24)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=24, out_features=24, bias=False)\n",
      "        (k_proj): Linear(in_features=24, out_features=12, bias=False)\n",
      "        (v_proj): Linear(in_features=24, out_features=12, bias=False)\n",
      "        (o_proj): Linear(in_features=24, out_features=24, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=24, out_features=96, bias=False)\n",
      "        (up_proj): Linear(in_features=24, out_features=96, bias=False)\n",
      "        (down_proj): Linear(in_features=96, out_features=24, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(raw_model)\n",
    "print(raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,896 || all params: 54,744 || trainable%: 30.8637\n"
     ]
    }
   ],
   "source": [
    "lora_model = copy.deepcopy(raw_model)\n",
    "replace_linear_with_lora(lora_model)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "seq_len = 8\n",
    "test_tensor = torch.randint(0, config.vocab_size, (bs, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = copy.deepcopy(raw_model)\n",
    "replace_linear_with_lora(lora_model, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 37,848 || all params: 37,848 || trainable%: 100.0000\n"
     ]
    }
   ],
   "source": [
    "raw_model.eval()\n",
    "print_trainable_parameters(raw_model)\n",
    "raw_res = raw_model(test_tensor).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,896 || all params: 54,744 || trainable%: 30.8637\n"
     ]
    }
   ],
   "source": [
    "# 第一次直接初始化 lora 的前向结果\n",
    "lora_model.eval()\n",
    "print_trainable_parameters(lora_model)  # 检查参数和可训练情况\n",
    "before_unload_res = lora_model(test_tensor).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 37,848 || all params: 37,848 || trainable%: 100.0000\n"
     ]
    }
   ],
   "source": [
    "# 卸载 lora 后的前向结果\n",
    "unload_lora(lora_model)\n",
    "lora_model.eval()\n",
    "print_trainable_parameters(lora_model)  # 检查参数和可训练情况\n",
    "unload_res = lora_model(test_tensor).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,896 || all params: 54,744 || trainable%: 30.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\柒\\AppData\\Local\\Temp\\ipykernel_14492\\1891864349.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_parameters = torch.load(f\"{adapter_name}.pt\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 重新装载 lora 后的前向结果\n",
    "load_lora(lora_model)\n",
    "lora_model.eval()\n",
    "print_trainable_parameters(lora_model)  # 检查参数和可训练情况\n",
    "load_res = lora_model(test_tensor).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(raw_res, unload_res, atol=1e-6))           # 应为 True\n",
    "print(torch.allclose(before_unload_res, load_res, atol=1e-6))   # 应为 True\n",
    "print(torch.allclose(raw_res, load_res, atol=1e-6))             # 应为 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
